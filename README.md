# Text Classification- Evaluating Three Classification Algorithms Across Two Distinct Datasets: A Comparative Analysis

This project was conducted as part of the Data Mining course within the first year of the Graduate Diploma in Computer and Information Science. Python programming language was utilised for the implementation.

The primary objective was to enhance the performance of three algorithms, Multinomial Naive Bayes, MLP and SVM and improve data quality by employing a wide range of preprocessing techniques. 
In the first experiment, I achieved the best scores with raw datasets by optimising models with various parameters and using feature engineering techniques. 
In the second experiment, I continued to employ the optimised algorithms, testing them with processed datasets using different preprocessing techniques. 
For the 'Fake and Real News' dataset, the preprocessing techniques used in the experiment included stop word removal, SpaCy lemmatisation, as well as CountVectorizer and TF-IDF for feature engineering that were compared with Embedding, and tuning the algorithms to find best parameters. 
Through iterative experiments with these techniques, I identified a model, preprocessing method, and feature engineering technique that produced the best outcome, achieving a 0.99  for the three evaluations metrics (Precision, Recall and F1-score). 
The report also compares the similarities and differences between the three algorithms, preprocessing techniques, and feature engineering techniques in detail which provides valuable insights for future projects in the realm of text classification and sentiment analysis.
